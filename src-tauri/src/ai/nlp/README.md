# Module `ai/nlp` ‚Äî Traitement du Langage Naturel

Ce module regroupe les outils de **bas niveau** pour la manipulation technique du texte. Contrairement au module `agents` qui g√®re le sens (s√©mantique), le module `nlp` g√®re la forme (syntaxe, tokens, vecteurs).

Il sert de biblioth√®que utilitaire transversale pour `llm` (gestion du contexte) et `context` (pr√©paration des donn√©es RAG).

## üéØ Objectifs

1.  **Tokenization** : Transformer le texte brut en tokens pour estimer la taille des prompts et √©viter de d√©passer la fen√™tre de contexte des mod√®les (ex: 4096 tokens pour Mistral).
2.  **Chunking (D√©coupage)** : Diviser intelligemment les documents longs en morceaux digestes pour le RAG.
3.  **Vectorisation (Embeddings)** : Transformer le texte en vecteurs math√©matiques (`Vec<f32>`) pour la recherche s√©mantique (via Qdrant/LEANN).

---

## üìÇ Architecture Pr√©vue

```mermaid
graph TD
    Input[Texte Brut] -->|Tokenizer| Tokens[Liste de Tokens]
    Tokens -->|Counter| Cost[Estimation Co√ªt/Taille]

    Input -->|Splitter| Chunks[Fragments de Texte]
    Chunks -->|Embedder| Vectors[Vecteurs (Float32)]

    Vectors --> VectorDB[(Vector DB / Qdrant)]
```

### 1\. `tokenizers` _(√Ä impl√©menter)_

Wrapper autour de la crate Rust `tokenizers` (HuggingFace).

- **Usage** : Avant d'envoyer une requ√™te √† `LlmClient`, on v√©rifie : `if count_tokens(prompt) > 4000 { error("Prompt trop long") }`.
- **Mod√®les support√©s** : BPE (Byte-Pair Encoding) compatible Llama/Mistral.

### 2\. `splitting` _(√Ä impl√©menter)_

Algorithmes de d√©coupage de texte.

- **Na√Øf** : D√©coupage par caract√®res (ex: tous les 1000 chars).
- **S√©mantique** : D√©coupage respectant les paragraphes (Markdown headers, sauts de ligne) pour ne pas couper une phrase en deux.
- **Overlap** : Gestion du chevauchement (ex: 10% de recouvrement entre deux chunks) pour pr√©server le contexte aux fronti√®res.

### 3\. `embeddings` _(√Ä impl√©menter)_

Interface pour g√©n√©rer des vecteurs.

- **Local** : Utilisation de `ort` (ONNX Runtime) avec un petit mod√®le type `all-MiniLM-L6-v2` (\~80MB) embarqu√© dans l'app.
- **Cloud** : Appel √† l'API Embeddings de Google/OpenAI (si mode Cloud activ√©).

---

## üîÑ Int√©gration dans le flux

### Flux actuel (v0.1.0)

Le module est passif. Le d√©coupage est fait sommairement dans `ai/context/retriever.rs`.

### Flux cible (v0.2.0)

1.  **L'Agent** g√©n√®re un prompt.
2.  **NLP** calcule les tokens : "Attention, il ne reste que 500 tokens pour la r√©ponse".
3.  **Context** r√©cup√®re un gros fichier de documentation.
4.  **NLP** le d√©coupe en chunks de 512 tokens.
5.  **NLP** vectorise ces chunks.
6.  **Context** cherche les 3 chunks les plus proches math√©matiquement de la question utilisateur.

---

## üõ†Ô∏è Stack Technique envisag√©e

- **Crate `tokenizers`** : Standard industriel, √©crit en Rust, tr√®s rapide.
- **Crate `text-splitter`** : Pour le chunking intelligent.
- **Crate `candle-core`** ou **`ort`** : Pour faire tourner des mod√®les d'embedding (BERT/MiniLM) directement en Rust sans Python.

---

## üìä √âtat d'Avancement

| Composant             | Statut     | Priorit√©                          |
| :-------------------- | :--------- | :-------------------------------- |
| **Token Counter**     | ‚ùå √Ä faire | Haute (pour robustesse LLM)       |
| **Markdown Splitter** | ‚ùå √Ä faire | Moyenne (pour RAG avanc√©)         |
| **ONNX Embedder**     | ‚ùå √Ä faire | Basse (pour recherche s√©mantique) |

---

> **Note :** Ce module est pour l'instant une coquille architecturale destin√©e √† accueillir la complexit√© croissante du traitement de texte au fur et √† mesure que RAISE montera en puissance.
