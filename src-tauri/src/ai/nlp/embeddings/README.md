# üß† NLP Embeddings Engine

Ce module g√®re la vectorisation de texte (Text Embedding), brique fondamentale du syst√®me RAG (Retrieval-Augmented Generation) de GenAptitude. Il transforme le langage naturel en vecteurs math√©matiques comparables.

## üèó Architecture

Le moteur utilise un **Pattern Strat√©gie** pour abstraire l'impl√©mentation sous-jacente. L'interface publique est fournie par `EmbeddingEngine` dans `mod.rs`.

### Moteurs Disponibles

Nous supportons deux backends d'inf√©rence, s√©lectionnables via l'enum `EngineType` :

#### 1. FastEmbed (D√©faut)

- **Fichier** : `fast.rs`
- **Technologie** : Runtime ONNX (via la crate `fastembed`).
- **Mod√®le** : `BAAI/bge-small-en-v1.5`.
- **Avantages** : Tr√®s rapide, optimis√©, t√©l√©chargement automatique des poids (quantized).
- **Usage** : Recommand√© pour le d√©veloppement et la production standard.

#### 2. Candle (Pure Rust)

- **Fichier** : `candle.rs`
- **Technologie** : Framework ML natif Rust de Hugging Face (`candle-core`, `candle-transformers`).
- **Mod√®le** : `sentence-transformers/all-MiniLM-L6-v2`.
- **Avantages** : Aucune d√©pendance syst√®me (pas de libonnx, pas de C++), id√©al pour la compilation crois√©e ou les environnements restreints.
- **Fonctionnement** : T√©l√©charge les poids `.safetensors` via `hf-hub`, tokenize, et ex√©cute le graphe BERT manuellement.

## üìÇ Structure des Fichiers

```text
src-tauri/src/ai/nlp/embeddings/
‚îú‚îÄ‚îÄ mod.rs       # Fa√ßade publique et dispatcher.
‚îú‚îÄ‚îÄ fast.rs      # Impl√©mentation ONNX (FastEmbed).
‚îî‚îÄ‚îÄ candle.rs    # Impl√©mentation Pure Rust (Candle/BERT).

```

## üöÄ Utilisation

```rust
use crate::ai::nlp::embeddings::{EmbeddingEngine, EngineType};

async fn example() -> Result<()> {
    // 1. Initialisation (T√©l√©charge les mod√®les au premier lancement)
    // Par d√©faut (FastEmbed) :
    let mut engine = EmbeddingEngine::new()?;

    // Ou sp√©cifiquement Candle :
    // let mut engine = EmbeddingEngine::new_with_type(EngineType::Candle)?;

    // 2. Vectorisation d'une requ√™te (pour la recherche)
    let query_vec = engine.embed_query("Comment cr√©er un acteur logique ?")?;
    println!("Vecteur de dimension : {}", query_vec.len()); // ex: 384

    // 3. Vectorisation par lot (pour l'indexation)
    let docs = vec![
        "L'ing√©nierie syst√®me est complexe.".to_string(),
        "Arcadia d√©finit 5 couches.".to_string()
    ];
    let batch_vecs = engine.embed_batch(docs)?;

    Ok(())
}

```

## üì¶ Gestion du Cache

Les mod√®les sont t√©l√©charg√©s automatiquement lors de la premi√®re ex√©cution :

- **FastEmbed** : Stock√© dans `src-tauri/.fastembed_cache/` (√† exclure de Git).
- **Candle** : Stock√© dans le cache standard Hugging Face (`~/.cache/huggingface/hub`).

## ‚ö†Ô∏è Notes Techniques

- **Mutabilit√©** : Les m√©thodes `embed_batch` et `embed_query` prennent `&mut self` car certains runtimes internes (ou tokenizers) peuvent n√©cessiter une mutabilit√© pour le cache interne ou les buffers.
- **Normalisation** : Les vecteurs de sortie sont normalis√©s (L2 Norm), ce qui permet d'utiliser le _Cosine Similarity_ via un simple produit scalaire (Dot Product).

```

---

```
