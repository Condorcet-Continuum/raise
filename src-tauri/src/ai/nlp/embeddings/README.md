# üß† NLP Embeddings Engine

Ce module g√®re la vectorisation de texte (Text Embedding), brique fondamentale du syst√®me RAG (Retrieval-Augmented Generation) de RAISE. Il transforme le langage naturel en vecteurs math√©matiques comparables pour permettre la recherche s√©mantique dans Qdrant.

## üèó Architecture

Le moteur utilise un **Pattern Strat√©gie** pour abstraire l'impl√©mentation sous-jacente. L'interface publique est fournie par `EmbeddingEngine` dans `mod.rs`. Il s√©lectionne automatiquement la meilleure impl√©mentation disponible selon l'environnement.

### Diagramme de Flux

```mermaid
graph TD
    User[App / RAG Service] -->|Appelle| Init[EmbeddingEngine::new]

    subgraph Selection_Strategy [Strat√©gie de S√©lection]
        Init --> TryCandle{Tentative Candle?}
        TryCandle -- "Succ√®s" --> CandleStr[Moteur Candle]
        TryCandle -- "√âchec (R√©seau/Conflit)" --> Fallback[Fallback]
        Fallback --> FastStr[Moteur FastEmbed]
    end

    subgraph Hardware_Abstraction [Acc√©l√©ration Mat√©rielle]
        CandleStr -- "macOS ARM" --> Metal[GPU Metal]
        CandleStr -- "Linux/Win + Nvidia" --> Cuda[GPU CUDA]
        CandleStr -- "Linux CPU" --> CPU_Rust[CPU Pure Rust]
        FastStr --> CPU_Onnx[CPU ONNX Runtime]
    end

    subgraph Pipeline [Traitement Vectoriel]
        Metal & Cuda & CPU_Rust & CPU_Onnx --> Token[Tokenization]
        Token --> Infer[Inf√©rence Mod√®le]
        Infer --> Norm[Normalisation L2]
    end

    Norm --> Output([Vecteur 384 dims])

```

### Moteurs Disponibles

#### 1. Candle (Pure Rust + GPU/Accelerate) - _Prioritaire_

- **Fichier** : `candle.rs`
- **Technologie** : Framework ML natif Rust de Hugging Face.
- **Mod√®le** : `sentence-transformers/all-MiniLM-L6-v2` (384 dimensions).
- **Performance** :
- **macOS (Apple Silicon)** : Utilise l'acc√©l√©ration **Metal** (tr√®s rapide).
- **Linux/Windows (Nvidia)** : Utilise **CUDA** (si configur√©).
- **Linux (CPU)** : Utilise **MKL/OpenBLAS** (via la feature `accelerate` si activ√©e, ou CPU pur).

- **Avantage** : Contr√¥le total, pas de d√©pendance Python/ONNX externe, support GPU natif.

#### 2. FastEmbed (ONNX Runtime) - _Fallback_

- **Fichier** : `fast.rs`
- **Technologie** : Runtime ONNX via la crate `fastembed`.
- **Mod√®le** : `BAAI/bge-small-en-v1.5` (384 dimensions).
- **Usage** : Utilis√© si Candle √©choue √† s'initialiser ou si l'utilisateur force ce mode. Tr√®s performant sur CPU standard.

## üìÇ Structure des Fichiers

```text
src-tauri/src/ai/nlp/embeddings/
‚îú‚îÄ‚îÄ mod.rs       # Fa√ßade publique et logique de s√©lection (Factory).
‚îú‚îÄ‚îÄ fast.rs      # Impl√©mentation CPU-Optimized (FastEmbed/ONNX).
‚îî‚îÄ‚îÄ candle.rs    # Impl√©mentation GPU-Capable (Candle/BERT).

```

## üöÄ Utilisation

```rust
use crate::ai::nlp::embeddings::{EmbeddingEngine, EngineType};

async fn example() -> Result<()> {
    // 1. Initialisation (Auto-d√©tection GPU/CPU)
    // T√©l√©charge les mod√®les automatiquement au premier lancement (~90 Mo)
    let mut engine = EmbeddingEngine::new()?;

    // 2. Vectorisation d'une requ√™te (pour la recherche)
    let query_vec = engine.embed_query("Comment cr√©er un acteur logique ?")?;
    println!("Vecteur de dimension : {}", query_vec.len()); // Toujours 384

    // 3. Vectorisation par lot (pour l'indexation massive)
    let docs = vec![
        "L'ing√©nierie syst√®me est complexe.".to_string(),
        "Arcadia d√©finit 5 couches.".to_string()
    ];
    let batch_vecs = engine.embed_batch(docs)?;

    Ok(())
}

```

## ‚öôÔ∏è Configuration (Cargo.toml)

La performance d√©pend des "features" activ√©es dans `src-tauri/Cargo.toml`.

### Pour macOS (Apple Silicon)

```toml
candle-core = { version = "...", features = ["metal", "accelerate"] }

```

### Pour Linux / Windows (NVIDIA)

```toml
# N√©cessite CUDA Toolkit install√© (nvcc)
candle-core = { version = "...", features = ["cuda"] }

```

### Pour Linux / Serveur (CPU Universel)

```toml
# Configuration par d√©faut la plus stable
candle-core = { version = "...", features = [] }

```

## üì¶ Gestion du Cache

Les mod√®les sont t√©l√©charg√©s automatiquement lors de la premi√®re ex√©cution :

- **Candle** : Cache standard Hugging Face (`~/.cache/huggingface/hub`).
- **FastEmbed** : Cache local `src-tauri/.fastembed_cache/` (ignor√© par Git).

## ‚ö†Ô∏è Notes Techniques

1. **Dimensions** : Les deux moteurs sont configur√©s pour sortir des vecteurs de taille **384**. C'est la taille standard pour les mod√®les "Small" (MiniLM, BGE-Small) qui offrent le meilleur compromis vitesse/pr√©cision pour du RAG local.
2. **Normalisation** : Les vecteurs de sortie sont **normalis√©s (L2 Norm)**. C'est crucial pour que la "Cosine Similarity" (utilis√©e par Qdrant) fonctionne correctement via un simple produit scalaire.
3. **Thread Safety** : L'instanciation du moteur peut prendre du temps (chargement mod√®le). Il est recommand√© de l'instancier une fois au d√©marrage de l'app (dans le `State` Tauri) et de le r√©utiliser.

```

```
