use crate::utils::prelude::*;

use candle_core::quantized::gguf_file;
use candle_core::{Device, Tensor};
use candle_transformers::generation::LogitsProcessor;
use candle_transformers::models::quantized_qwen2 as model;
use tokenizers::Tokenizer;

pub struct CandleLlmEngine {
    model: model::ModelWeights,
    tokenizer: Tokenizer,
    device: Device,
    logits_processor: LogitsProcessor,
}

impl CandleLlmEngine {
    pub async fn new(
        manager: &crate::json_db::collections::manager::CollectionsManager<'_>,
    ) -> RaiseResult<Self> {
        // 1. R√©cup√©ration de la configuration globale dynamique
        let settings =
            crate::utils::config::AppConfig::get_component_settings(manager, "llm").await?;

        // 2. Lecture simple des param√®tres
        let model_filename = settings
            .get("rust_model_file")
            .and_then(|v| v.as_str())
            .unwrap_or("qwen2.5-1.5b-instruct-q4_k_m.gguf");
        let tokenizer_filename = settings
            .get("rust_tokenizer_file")
            .and_then(|v| v.as_str())
            .unwrap_or("tokenizer.json");

        // 3. Construction des chemins LOCAUX absolus (100% Hors-ligne)
        let Some(home) = dirs::home_dir() else {
            raise_error!(
                "ERR_OS_HOME_NOT_FOUND",
                error = "Impossible de localiser le r√©pertoire personnel de l'utilisateur (HOME).",
                context = json!({ "method": "dirs::home_dir" })
            );
        };

        let base_path = home.join("raise_domain/_system/ai-assets/models");
        let model_path = base_path.join(model_filename);
        let tokenizer_path = base_path.join(tokenizer_filename);

        // 4. V√©rification de s√©curit√© stricte
        if !model_path.exists() {
            raise_error!(
                "ERR_AI_MODEL_FILE_NOT_FOUND",
                error = format!("Mod√®le GGUF introuvable en local : {:?}", model_path),
                context = json!({ "path": model_path.to_string_lossy() })
            );
        }
        if !tokenizer_path.exists() {
            raise_error!(
                "ERR_AI_TOKENIZER_FILE_NOT_FOUND",
                error = format!("Tokenizer introuvable en local : {:?}", tokenizer_path),
                context = json!({ "path": tokenizer_path.to_string_lossy() })
            );
        }

        // 5. Initialisation mat√©rielle (CPU par d√©faut)
        // 5. D√âTECTION MAT√âRIELLE DYNAMIQUE (Priorit√© absolue : CUDA)
        let device = candle_core::Device::new_cuda(0).unwrap_or(candle_core::Device::Cpu); // Fallback CPU si erreur

        println!("üöÄ [Candle LLM] Moteur Qwen charg√© sur : {:?}", device);

        // 6. Chargement du Tokenizer depuis le fichier local
        let tokenizer = match tokenizers::Tokenizer::from_file(&tokenizer_path) {
            Ok(t) => t,
            Err(e) => {
                // La macro g√®re d√©j√† le retour divergent
                raise_error!(
                    "ERR_AI_TOKENIZER_LOAD_FAILED",
                    error = e,
                    context = json!({
                        "action": "initialize_tokenizer",
                        "path": tokenizer_path.to_string_lossy(),
                        "hint": "Le fichier 'tokenizer.json' est introuvable ou malform√©. V√©rifiez que le mod√®le a √©t√© correctement t√©l√©charg√© dans le dossier 'assets/models'."
                    })
                )
            }
        };

        // 7. Chargement du mod√®le GGUF depuis le fichier local
        let mut file = match std::fs::File::open(&model_path) {
            Ok(f) => f,
            Err(e) => raise_error!(
                "ERR_AI_MODEL_OPEN",
                error = e,
                context = json!({ "path": model_path.to_string_lossy() })
            ),
        };

        let model = match gguf_file::Content::read(&mut file) {
            Ok(m) => m,
            Err(e) => raise_error!(
                "ERR_AI_MODEL_READ_CONTENT",
                error = e,
                context = json!({
                    "action": "READ_GGUF_CONTENT",
                    "path": model_path.to_string_lossy()
                })
            ),
        };
        // 8. Initialisation de l'architecture Qwen2
        let weights = match candle_transformers::models::quantized_qwen2::ModelWeights::from_gguf(
            model, &mut file, &device,
        ) {
            Ok(w) => w,
            Err(e) => raise_error!(
                "ERR_AI_QWEN2_WEIGHTS_LOAD",
                error = e,
                context = json!({
                    "model_family": "Qwen2",
                    "path": model_path.to_string_lossy()
                })
            ),
        }; // 9. Initialisation du processeur de texte (Temp√©rature 0.7 par d√©faut)
        let logits_processor = LogitsProcessor::new(299792458, Some(0.7), None);

        // Retour de l'instance
        Ok(Self {
            model: weights,
            tokenizer,
            device,
            logits_processor, // üéØ On n'oublie pas de le retourner !
        })
    }

    fn format_prompt(system_prompt: &str, user_prompt: &str) -> String {
        // Format ChatML utilis√© par Qwen 2.5
        format!(
            "<|im_start|>system\n{}<|im_end|>\n<|im_start|>user\n{}<|im_end|>\n<|im_start|>assistant\n",
            system_prompt, user_prompt
        )
    }

    pub fn generate(
        &mut self,
        system_prompt: &str,
        user_prompt: &str,
        max_tokens: usize,
    ) -> RaiseResult<String> {
        let formatted_prompt = Self::format_prompt(system_prompt, user_prompt);

        let tokens = match self.tokenizer.encode(formatted_prompt.as_str(), true) {
            Ok(t) => t,
            Err(e) => raise_error!(
                "ERR_TOKENIZER_ENCODE",
                error = e,
                context = json!({
                    "action": "encode_prompt",
                    // Un petit plus pour l'IA : on capture un aper√ßu du prompt qui a fait planter !
                    "prompt_preview": formatted_prompt.chars().take(50).collect::<String>()
                })
            ),
        };

        let mut tokens = tokens.get_ids().to_vec();
        let mut generated_tokens = Vec::new();

        let mut index_pos = 0;
        // Token standard de fin de message pour ChatML / Qwen
        let eos_token_id = self.tokenizer.token_to_id("<|im_end|>").unwrap_or(151645);
        // Token global de fin de texte pour Qwen
        let stop_token_id = self
            .tokenizer
            .token_to_id("<|endoftext|>")
            .unwrap_or(151643);

        for _i in 0..max_tokens {
            let context_size = if index_pos == 0 { tokens.len() } else { 1 };
            let start_pos = tokens.len().saturating_sub(context_size);

            // 1. Tenseur d'entr√©e
            let input = match Tensor::new(&tokens[start_pos..], &self.device) {
                Ok(t) => t,
                Err(e) => raise_error!(
                    "ERR_AI_TENSOR_CREATION_FAILED",
                    error = e,
                    context = json!({ "pos": index_pos })
                ),
            };

            let input = match input.unsqueeze(0) {
                Ok(t) => t,
                Err(e) => raise_error!("ERR_AI_TENSOR_SHAPE_ERROR", error = e),
            };

            // 2. Forward Pass
            let logits = match self.model.forward(&input, index_pos) {
                Ok(l) => l,
                Err(e) => raise_error!(
                    "ERR_AI_FORWARD_PASS_FAILED",
                    error = e,
                    context = json!({ "index_pos": index_pos })
                ),
            };

            // 3. Post-traitement (Extraction manuelle pour √©viter le mismatch)
            let logits = match logits.squeeze(0) {
                Ok(l) => l,
                Err(e) => raise_error!("ERR_AI_TENSOR_SQUEEZE_FAILED", error = e),
            };

            let logits = match logits.squeeze(0) {
                Ok(l) => l,
                Err(e) => raise_error!("ERR_AI_TENSOR_SQUEEZE_FAILED", error = e),
            };

            let logits = match logits.to_dtype(candle_core::DType::F32) {
                Ok(l) => l,
                Err(e) => raise_error!("ERR_AI_DTYPE_CONVERSION_FAILED", error = e),
            };

            // 4. Sampling
            let next_token = match self.logits_processor.sample(&logits) {
                Ok(t) => t,
                Err(e) => raise_error!("ERR_AI_SAMPLING_FAILED", error = e),
            };

            if next_token == eos_token_id || next_token == stop_token_id {
                break;
            }

            tokens.push(next_token);
            generated_tokens.push(next_token);
            index_pos += context_size;
        }

        let result = match self.tokenizer.decode(&generated_tokens, true) {
            Ok(res) => res,
            Err(e) => raise_error!(
                "ERR_TOKENIZER_DECODE",
                error = e,
                context = json!({
                    "action": "decode_tokens",
                    // Info ultra-utile pour l'IA/Debug : combien de tokens ont fait planter le d√©codeur ?
                    "token_count": generated_tokens.len()
                })
            ),
        };
        Ok(result)
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_qwen_chatml_format() {
        let sys = "Sys";
        let user = "User";
        let expected = "<|im_start|>system\nSys<|im_end|>\n<|im_start|>user\nUser<|im_end|>\n<|im_start|>assistant\n";
        assert_eq!(CandleLlmEngine::format_prompt(sys, user), expected);
    }

    #[tokio::test]
    #[serial_test::serial] // Protection RTX 5060 en local
    #[cfg_attr(not(feature = "cuda"), ignore)]
    async fn test_quick_inference() {
        crate::utils::config::test_mocks::inject_mock_config();
        let config = crate::utils::config::AppConfig::get();
        let storage_cfg = crate::json_db::storage::JsonDbConfig::new(
            config.get_path("PATH_RAISE_DOMAIN").unwrap(),
        );
        let storage = crate::json_db::storage::StorageEngine::new(storage_cfg);
        let manager = crate::json_db::collections::manager::CollectionsManager::new(
            &storage,
            &config.system_domain,
            &config.system_db,
        );
        manager.init_db().await.unwrap();

        // Appel magique de ton Mock Helper !
        crate::utils::config::test_mocks::inject_mock_component(
            &manager,
            "llm",
            crate::utils::json::json!({ "rust_tokenizer_file": "tokenizer.json", "rust_model_file": "qwen2.5-1.5b-instruct-q4_k_m.gguf" })
        ).await;

        // On passe le manager au moteur
        let mut engine = CandleLlmEngine::new(&manager)
            .await
            .expect("Echec Init Engine");

        println!("ü§ñ Generating...");
        let response = engine
            .generate("R√©ponds 'OK'.", "Tu m'entends ?", 10)
            .expect("Echec Generation");

        println!("üìù R√©ponse: {}", response);
        assert!(!response.is_empty());
    }
}
