# Module `ai::llm` - Infrastructure Bas Niveau LLM

Ce module constitue la couche d'infrastructure (**Low-Level Layer**) de RAISE pour la communication avec les mod√®les de langage. Il fournit la "tuyauterie" technique permettant aux services de fonctionner sans se soucier de la complexit√© r√©seau ou de l'inf√©rence locale.

Il supporte d√©sormais deux modes de fonctionnement :

1.  **Client HTTP (Agnostique)** : Pour connecter des serveurs d'inf√©rence externes (llama.cpp, vLLM) ou Cloud (Gemini).
2.  **Moteur Natif (Embedded)** : Pour ex√©cuter des mod√®les (GGUF) directement dans le processus Rust via `Candle` (sans d√©pendance externe).

---

## üìÇ Structure du Module

Voici l'organisation physique des fichiers de ce module :

```text
src-tauri/src/ai/llm/
‚îú‚îÄ‚îÄ mod.rs               # Point d'entr√©e : expose les structures et g√®re l'√©tat global (NativeLlmState).
‚îú‚îÄ‚îÄ client.rs            # Client HTTP : g√®re la connexion r√©seau (llama.cpp/Gemini) et le Fallback.
‚îú‚îÄ‚îÄ candle_engine.rs     # [NOUVEAU] Moteur Natif : Inf√©rence locale pure via HuggingFace Candle.
‚îú‚îÄ‚îÄ prompts.rs           # Personas : contient les constantes des "System Prompts".
‚îú‚îÄ‚îÄ response_parser.rs   # Nettoyeur : extrait le JSON/Code des r√©ponses brutes.
‚îî‚îÄ‚îÄ tests.rs             # Validation : tests unitaires et d'int√©gration.

```

---

## üìä Architecture & Flux de Donn√©es

Le syst√®me est **hybride**. Il permet de choisir le bon outil pour la bonne t√¢che.

### Sch√©ma du Flux (Pipeline)

```mermaid
graph TD
    User[Interface / Agent] --> Decision{Choix Architecture}

    %% BRANCHE 1 : CLIENT HTTP (AGENTS)
    Decision -- "Mode R√©seau (Agents complexes)" --> Client[LLM Client]

    subgraph Network_Flow [Flux Client HTTP]
        direction TB
        Client --> TryLocal[Tentative Local :8081]
        TryLocal -- "Timeout / √âchec" --> Fallback[Secours Cloud Gemini]
        TryLocal --> RawResp[R√©ponse Brute]
        Fallback --> RawResp
    end

    %% BRANCHE 2 : MOTEUR NATIF (CHAT)
    Decision -- "Mode Natif (Chat Rapide)" --> Engine[Candle Engine]

    subgraph Native_Flow [Flux Embarqu√© Rust]
        direction TB
        Engine --> Load[Chargement GGUF RAM]
        Load --> Infer[Inf√©rence Metal/CUDA/CPU]
        Infer --> Tokenizer[D√©codage Tokenizer]
    end

    %% CONVERGENCE ET SORTIE
    RawResp --> Parser[Response Parser]
    Tokenizer --> Output[Sortie Texte Standardis√©e]
    Parser --> Output

```

### Description des Moteurs

1. **Le Client HTTP (`client.rs`)** :

- Utilis√© par les **Agents Autonomes** (Software, Intent, etc.).
- Avantage : Peut utiliser des mod√®les √©normes (70B+) h√©berg√©s sur un serveur d√©di√© ou dans le Cloud.
- R√©silience : Bascule sur Gemini si le serveur local est √©teint.

2. **Le Moteur Natif (`candle_engine.rs`)** :

- Utilis√© par le **Chat Direct** ou les t√¢ches rapides.
- Avantage : **Z√©ro configuration**. Pas besoin de Docker ni de Python. L'application t√©l√©charge et lance le mod√®le (ex: Llama 3.2 1B) toute seule.
- Performance : Utilise l'acc√©l√©ration mat√©rielle (Metal sur Mac, CUDA sur Nvidia, AVX sur CPU).

---

## üíª Exemples d'Utilisation (Rust)

### Cas 1 : Via le Client HTTP (Agents)

Utilis√© pour les t√¢ches complexes n√©cessitant un mod√®le puissant distant.

```rust
use crate::ai::llm::{client, prompts, response_parser};

async fn classify_user_request(user_input: &str) -> Result<serde_json::Value, String> {
    // 1. Initialisation
    let llm_client = client::LlmClient::new( );

    // 2. Prompting
    let full_prompt = format!("{}\nREQ: {}", prompts::INTENT_CLASSIFIER_PROMPT, user_input);

    // 3. Appel R√©seau
    let raw_response = llm_client.ask_raw(&full_prompt).await.map_err(|e| e.to_string())?;

    // 4. Parsing
    let json_data = response_parser::extract_json(&raw_response).map_err(|e| e.to_string())?;
    Ok(json_data)
}

```

### Cas 2 : Via le Moteur Natif (Embedded)

Utilis√© pour interagir avec le mod√®le charg√© en m√©moire (State Tauri).

```rust
use crate::ai::llm::NativeLlmState;
use tauri::State;

#[tauri::command]
pub async fn chat_with_local_model(
    state: State<'_, NativeLlmState>,
    prompt: String
) -> Result<String, String> {
    // 1. R√©cup√©ration du verrou (Mutex)
    let mut guard = state.0.lock().map_err(|_| "Erreur Lock".to_string())?;

    // 2. V√©rification si le mod√®le est charg√©
    if let Some(engine) = guard.as_mut() {
        // 3. G√©n√©ration directe (In-Process)
        // Pas de r√©seau, pas de JSON, c'est du "Raw Text"
        engine.generate("Tu es un assistant.", &prompt, 200)
            .map_err(|e| e.to_string())
    } else {
        Err("Le mod√®le charge encore...".to_string())
    }
}

```

---

## ‚öôÔ∏è Configuration Requise

Variables d'environnement (fichier `.env`) :

### Configuration Client HTTP (Agents)

| Variable           | Description                                                                  |
| ------------------ | ---------------------------------------------------------------------------- |
| `RAISE_LOCAL_URL`  | URL du serveur d'inf√©rence local (ex: `http://localhost:8081`)               |
| `LLM_MODEL_FILE`   | Fichier mod√®le charg√© par le serveur (ex: qwen2.5-1.5b-instruct-q4_k_m.gguf) |
| `RAISE_GEMINI_KEY` | Cl√© API Google (Backup)                                                      |

### Configuration Moteur Natif (Rust/Candle)

Si ces variables ne sont pas d√©finies, des valeurs par d√©faut (Llama 3.2 1B) sont utilis√©es.

| Variable                  | Description                      | Exemple / Recommand√©                   |
| ------------------------- | -------------------------------- | -------------------------------------- |
| `LLM_RUST_REPO_ID`        | D√©p√¥t HuggingFace du mod√®le GGUF | `bartowski/Llama-3.2-1B-Instruct-GGUF` |
| `LLM_RUST_MODEL_FILE`     | Nom du fichier GGUF sp√©cifique   | `Llama-3.2-1B-Instruct-Q4_K_M.gguf`    |
| `LLM_RUST_TOKENIZER_REPO` | D√©p√¥t contenant `tokenizer.json` | `unsloth/Llama-3.2-1B-Instruct`        |

---

## ‚úÖ Validation

### Tester la logique g√©n√©rale

Pour v√©rifier le parser et le client HTTP :

```bash
cargo test ai::llm

```

### Tester le Moteur Natif (T√©l√©chargement + Inf√©rence)

**Attention :** Ce test t√©l√©charge le mod√®le (~700 Mo) lors de la premi√®re ex√©cution.

```bash
cargo test candle_engine -- --ignored

```

```

```
