# Module `ai/llm` ‚Äî Client d'Inf√©rence Unifi√© (Dual Mode)

Ce module est la passerelle de communication bas niveau entre GenAptitude et les Large Language Models (LLMs).

Il impl√©mente une architecture **Dual Mode** qui permet de basculer dynamiquement, requ√™te par requ√™te, entre une ex√©cution souveraine (locale) et une ex√©cution haute performance (cloud), le tout derri√®re une interface Rust unifi√©e.

## üèóÔ∏è Architecture Technique

Le module agit comme un **Adaptateur Universel**. Le reste de l'application (Agents, Commandes) ne se soucie pas du format JSON sp√©cifique de chaque fournisseur.

```mermaid
graph LR
    Caller[Agent / Command] -->|ask(backend, prompt)| Client[LlmClient]

    Client -->|Switch Backend| Router{Routeur}

    %% Branche Locale
    Router -- LocalLlama --> Adapter1[OpenAI Adapter]
    Adapter1 -->|POST /v1/chat/completions| Docker[üê≥ Docker (Mistral 7B)]
    Docker --> GPU[Nvidia GTX/RTX]

    %% Branche Cloud
    Router -- GoogleGemini --> Adapter2[Google REST Adapter]
    Adapter2 -->|POST /v1beta/models/generateContent| Cloud[‚òÅÔ∏è Google Vertex AI]

    %% Retour
    Docker -->|JSON| Parser[Response Unifier]
    Cloud -->|JSON| Parser

    Parser -->|String| Caller
```

---

## üìÇ Composants

### 1\. `client.rs` (Le Driver)

C'est le c≈ìur du module. Il encapsule toute la complexit√© r√©seau et protocolaire.

- **Gestionnaire HTTP** : Utilise `reqwest` avec des timeouts configur√©s (5 minutes) pour supporter les temps de g√©n√©ration longs des LLMs sur CPU/GPU local.
- **DTOs (Data Transfer Objects)** : Structures `Serialize`/`Deserialize` internes qui mappent les formats propri√©taires :
  - `OpenAiRequest` : Format standard (llama.cpp, Ollama, vLLM).
  - `GeminiRequest` : Format sp√©cifique Google (contents, parts, system_instruction).
- **Logique Unifi√©e** : La m√©thode `ask()` prend en charge le formatage du prompt syst√®me et utilisateur, l'envoi, et l'extraction propre du texte dans la r√©ponse.

### 2\. `mod.rs`

Point d'entr√©e du module. Expose les structures publiques (`LlmClient`, `LlmBackend`) et contient les tests d'int√©gration.

---

## ‚öôÔ∏è Configuration

Le client est "stateless" mais sa configuration est inject√©e au d√©marrage via les variables d'environnement (charg√©es par `dotenvy`).

| Variable                 | R√¥le                                          | Exemple                 |
| :----------------------- | :-------------------------------------------- | :---------------------- |
| `GENAPTITUDE_LOCAL_URL`  | Adresse du serveur d'inf√©rence local (Docker) | `http://localhost:8080` |
| `GENAPTITUDE_GEMINI_KEY` | Cl√© API Google AI Studio (Optionnel)          | `AIzaSy...`             |
| `GENAPTITUDE_MODEL_NAME` | Nom du mod√®le Cloud cible                     | `gemini-1.5-pro`        |

---

## üöÄ Guide d'Utilisation (Rust)

### Instanciation

Le client est con√ßu pour √™tre instanci√© une fois au niveau de la commande ou du CLI, puis clon√© (le clone est l√©ger, c'est juste un pointeur vers le pool de connexions).

```rust
use crate::ai::llm::client::{LlmClient, LlmBackend};

// Configuration charg√©e depuis l'env
let client = LlmClient::new(
    "http://localhost:8080",
    "ma_cle_google",
    Some("gemini-1.5-pro".to_string())
);
```

### Appel Unifi√© (`ask`)

La m√©thode `ask` est asynchrone et retourne un `Result<String>`.

**Cas 1 : Rapidit√© & Confidentialit√© (Local)**
_Pour la classification d'intention, le chat simple, les petites corrections._

```rust
let response = client.ask(
    LlmBackend::LocalLlama,
    "Tu es un expert Rust.",      // System Prompt
    "Comment faire une struct ?"  // User Prompt
).await?;
```

**Cas 2 : Intelligence Complexe (Cloud)**
_Pour la g√©n√©ration de SML, l'analyse d'architecture, ou quand le GPU local sature._

```rust
let response = client.ask(
    LlmBackend::GoogleGemini,
    "Tu es un architecte syst√®me senior.",
    "Analyse les incoh√©rences de ce mod√®le complexe..."
).await?;
```

---

## üõ°Ô∏è S√©curit√© et Robustesse

1.  **Isolation des Donn√©es** :

    - En mode `LocalLlama`, **aucune donn√©e ne quitte la machine**. Les paquets restent sur la boucle locale (`localhost`).
    - C'est le mode par d√©faut et privil√©gi√© pour les donn√©es sensibles.

2.  **Gestion d'Erreurs (Fail-fast)** :

    - Le client v√©rifie les statuts HTTP (`!res.status().is_success()`) avant de tenter de parser le JSON.
    - Les erreurs r√©seau (Docker √©teint, Internet coup√©) sont propag√©es via `anyhow` pour √™tre affich√©es proprement √† l'utilisateur.

3.  **Parsing R√©silient** :

    - Utilisation de `Option<T>` pour les champs de r√©ponse JSON qui peuvent manquer selon les versions d'API.

---

## üîÆ Roadmap Technique

- [ ] **Streaming (SSE)** : Impl√©menter `ask_stream()` pour recevoir la r√©ponse token par token (effet "machine √† √©crire" dans l'UI).
- [ ] **Embeddings** : Ajouter une m√©thode `embed(text) -> Vec<f32>` pour vectoriser le texte (n√©cessaire pour le futur moteur de recherche s√©mantique).
- [ ] **Token Counting** : Estimer le nombre de tokens avant envoi pour √©viter les erreurs "Context Length Exceeded".
- [ ] **Fallback Automatique** : Si le Cloud est inaccessible (timeout/erreur), basculer automatiquement sur le Local en mode d√©grad√©.
