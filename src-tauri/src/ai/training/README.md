# üéì AI Training & Fine-Tuning Module (Native Rust)

Ce module g√®re la pr√©paration des donn√©es et l'entra√Ænement **Fine-Tuning** au sein de l'application. Il utilise **Candle** (HuggingFace) pour un entra√Ænement QLoRA natif, permettant de sp√©cialiser l'IA par domaine m√©tier sans d√©pendre de Python.

## üìÇ Structure du Module et Arborescence

L'organisation des fichiers suit une s√©paration stricte entre la gestion des donn√©es, la structure math√©matique des mod√®les et l'orchestration Tauri :

```text
src-tauri/src/ai/training/
‚îú‚îÄ‚îÄ mod.rs              # Point d'entr√©e, commandes Tauri et orchestration
‚îú‚îÄ‚îÄ dataset.rs          # Extraction, filtrage par domaine et formatage JSON-DB
‚îú‚îÄ‚îÄ lora.rs             # Impl√©mentation technique des couches LoraLinear (Matrices A & B)
‚îî‚îÄ‚îÄ README.md           # Documentation technique et math√©matique

```

### R√¥le des fichiers :

- **`mod.rs`** : Contient la commande `ai_train_domain_native`. Il initialise le `VarMap`, configure l'optimiseur AdamW et g√®re la boucle d'entra√Ænement ainsi que la sauvegarde finale des adaptateurs.
- **`dataset.rs`** : G√®re l'interface avec le `StorageEngine`. Il filtre les collections de la base de donn√©es selon le domaine (ex: "safety") et transforme les documents bruts en structures `TrainingExample` (Instruction/Input/Output).
- **`lora.rs`** : D√©finit la logique des tenseurs. C'est ici qu'est inject√©e la branche de bas rang qui permet l'apprentissage sans modifier les poids originaux du mod√®le.

## üèóÔ∏è Architecture du Syst√®me

Le flux de travail est enti√®rement int√©gr√© au backend Rust :

```mermaid
graph TD
    subgraph "Couche Donn√©es (JSON-DB)"
        DB[(Fichiers .json par domaine)]
        DS[dataset.rs: extract_domain_data]
        DB -->|Filtrage Domaine| DS
    end

    subgraph "Pr√©paration (In-Memory)"
        EX[TrainingExamples]
        TK[NLP Module: Tokenizer]
        DS --> EX
        EX --> TK
    end

    subgraph "Moteur Candle (lora.rs)"
        direction TB
        input[Input Tensors]
        subgraph "LoraLinear Layer"
            W[W: Frozen Weights 4-bit]
            A[Matrice A: Entra√Ænable]
            B[Matrice B: Entra√Ænable]
        end
        TK --> input
        input --> W
        input --> B
        B --> A
        W --> Sum[Addition + Scale]
        A --> Sum
    end

    subgraph "Sortie"
        LOSS[Calcul de Loss: CrossEntropy]
        OPT[Optimiseur: AdamW]
        SAFE[Export: .safetensors]

        Sum --> LOSS
        LOSS --> OPT
        OPT -->|Update Gradients| A
        OPT -->|Update Gradients| B
        OPT -->|Final Save| SAFE
    end

```

## ‚ûó D√©tails Math√©matiques : L'op√©ration LoRA

L'adaptation de bas rang (LoRA) permet de ne pas modifier la matrice de poids originale (gel√©e), mais d'ajouter une d√©viation apprise par deux matrices plus petites et .

Pour une entr√©e de dimension , le calcul se d√©compose ainsi :

1. **Branche Standard (Frozen) :**
   Calcul de la sortie classique du mod√®le :

2. **Branche LoRA (Adaptation) :**
   L'entr√©e subit une r√©duction puis une projection :

- **R√©duction () :** (o√π est de dimension ). R√©sultat : .
- **Projection () :** (o√π est de dimension ). R√©sultat : .

3. **Combinaison :**

Le facteur `scale` est d√©fini par .

## üöÄ Utilisation (Tauri)

L'entra√Ænement est d√©clench√© depuis le frontend par domaine m√©tier :

```typescript
await invoke('ai_train_domain_native', {
  space: 'Projet_Arcadia',
  dbName: 'main_db',
  domain: 'safety',
  epochs: 5,
  lr: 0.0001,
});
```

## üß™ Validation et Tests

Le module inclut des tests unitaires pour garantir la stabilit√© :

- **Dimensions** : V√©rifie que le calcul matriciel respecte les formes .
- **Filtrage** : Valide que seules les collections du domaine demand√© sont extraites de JSON-DB.

```bash
cargo test ai::training

```

---
