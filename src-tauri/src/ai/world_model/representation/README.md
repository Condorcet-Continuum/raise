# Module Repr√©sentation (L'Espace Latent)

Le sous-module **Repr√©sentation** est le c≈ìur de l'abstraction. Il structure l'information per√ßue pour la rendre manipulable par des algorithmes de raisonnement.

Il est impl√©ment√© dans `quantizer.rs`.

## üéØ Objectif

Transformer les vecteurs continus (flous) issus de la _Perception_ en symboles discrets (concepts). C'est ce qui permet au World Model de "parler" un langage proche de celui du `model_engine`.

> **Analogie :** C'est la zone du cerveau qui classifie. Elle ne voit plus "une forme grise avec 4 pattes" (Perception), elle reconna√Æt le concept "Chat" (Repr√©sentation).

## Technologie : Vector Quantization (VQ)

Nous utilisons une **Quantification Vectorielle** (approche VQ-VAE) pour discr√©tiser l'espace latent.

### Algorithme (`VectorQuantizer`)

L'impl√©mentation repose sur la recherche du plus proche voisin dans un dictionnaire de vecteurs appris (Codebook).

1.  **Le Codebook (Embedding Table) :**
    Stock√© sous forme de matrice `[K, D]` via `candle_nn::Embedding`.

    - $K$ : Nombre de concepts (Vocabulaire).
    - $D$ : Dimension des vecteurs (Embedding Dim).

2.  **Tokenization (Encodage) :**
    Pour un vecteur d'entr√©e $z$, on calcule la distance avec tous les vecteurs du codebook $e_i$ :
    $$k = \text{argmin}_i \|z - e_i\|^2$$
    _Optimisation math√©matique :_ Le calcul utilise la d√©composition $\|z - e\|^2 = \|z\|^2 + \|e\|^2 - 2\langle z, e \rangle$ pour profiter des acc√©l√©rations matricielles.

3.  **D√©codage (Reconstruction) :**
    √Ä partir d'un index $k$, on r√©cup√®re le vecteur prototype $e_k$.

## Pourquoi c'est vital pour RAISE ?

L'ing√©nierie syst√®me repose sur des √©tats discrets (Valid√©/Non-Valid√©, Connect√©/D√©connect√©).

- Un r√©seau de neurones standard sort une probabilit√© continue (ex: `0.98`).
- Ce module force cette sortie √† devenir un √©tat ferme (ex: `State::Connected`, Token #42).
- Cela permet d'appliquer les **Validateurs** symboliques de `model_engine` directement sur les pr√©dictions de l'IA.

## Flux de Donn√©es

```mermaid
graph LR
    Input["Vecteur Continu\n(Sortie Perception)"] -->|Tokenize| VQ[Vector Quantizer]

    subgraph "M√©canique Interne"
        VQ --"Distance L2"--> Search{Recherche Voisin}
        Codebook[("Codebook\n(Matrice Embedding)")] -.-> Search
    end

    Search -->|Argmin| Token["Token Index (Entier)\n(Concept Discret)"]
    Token -->|Decode| Output["Vecteur Prototype\n(Entr√©e Dynamique)"]
```

## Impl√©mentation Technique

- **Fichier :** `src-tauri/src/ai/world_model/representation/quantizer.rs`
- **Struct :** `VectorQuantizer`
- **D√©pendances :** `candle-nn` (Module Embedding).

## √âvolutions Futures

- **Codebook dynamique :** Ajouter des concepts au dictionnaire √† la vol√©e si l'erreur de reconstruction d√©passe un seuil (Apprentissage continu de nouveaux concepts).
- **Entra√Ænement :** Impl√©menter le "Straight-Through Estimator" pour permettre la r√©tro-propagation du gradient √† travers l'op√©ration discr√®te `argmin`.

```

```
