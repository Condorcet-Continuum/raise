import torch
import os
import sys
from datasets import load_dataset
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
)
from trl import SFTTrainer

# --- CONFIGURATION PAR D√âFAUT ---
# Vous pourrez changer ces valeurs ou les passer en arguments plus tard
MODEL_ID = "mistralai/Mistral-7B-Instruct-v0.2" 
NEW_MODEL_NAME = "genaptitude-lora-adapter"
DATASET_FILE = "dataset.jsonl" 

def train():
    print(f"üöÄ Initialisation de l'entra√Ænement QLoRA sur {MODEL_ID}")

    # 1. V√©rification du Dataset
    if not os.path.exists(DATASET_FILE):
        print(f"‚ùå Erreur: Le fichier de donn√©es '{DATASET_FILE}' est introuvable.")
        print("   Veuillez d'abord exporter les donn√©es depuis GenAptitude.")
        sys.exit(1)

    # 2. Configuration QLoRA (4-bit Quantization)
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
    )

    # 3. Chargement du Mod√®le de base
    print("‚è≥ Chargement du mod√®le (peut prendre du temps)...")
    try:
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_ID,
            quantization_config=bnb_config,
            device_map="auto" # Utilise le GPU si disponible
        )
        model.config.use_cache = False
        model.config.pretraining_tp = 1
    except Exception as e:
        print(f"‚ùå Erreur chargement mod√®le: {e}")
        sys.exit(1)

    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"

    # 4. Configuration LoRA (Low-Rank Adaptation)
    peft_config = LoraConfig(
        lora_alpha=16,
        lora_dropout=0.1,
        r=64, # Rank: plus √©lev√© = plus de param√®tres apprenables (max 128 recommand√©)
        bias="none",
        task_type="CAUSAL_LM",
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj"]
    )

    model = prepare_model_for_kbit_training(model)
    model = get_peft_model(model, peft_config)

    # 5. Chargement des Donn√©es
    print(f"üìÇ Chargement du dataset: {DATASET_FILE}")
    dataset = load_dataset("json", data_files=DATASET_FILE, split="train")

    # 6. Param√®tres d'entra√Ænement
    training_args = TrainingArguments(
        output_dir="./results",
        num_train_epochs=1,          # Nombre de passes sur les donn√©es
        per_device_train_batch_size=4,
        gradient_accumulation_steps=1,
        optim="paged_adamw_32bit",   # Optimiseur √©conome en m√©moire
        save_steps=50,
        logging_steps=10,
        learning_rate=2e-4,
        weight_decay=0.001,
        fp16=True,
        bf16=False,
        max_grad_norm=0.3,
        max_steps=-1,
        warmup_ratio=0.03,
        group_by_length=True,
        lr_scheduler_type="constant",
    )

    # 7. Lancement du Trainer (Supervised Fine-Tuning)
    trainer = SFTTrainer(
        model=model,
        train_dataset=dataset,
        peft_config=peft_config,
        dataset_text_field="text", # Le champ JSON contenant le prompt format√©
        max_seq_length=None,
        tokenizer=tokenizer,
        args=training_args,
        packing=False,
    )

    print("üî• D√©marrage du Fine-Tuning...")
    trainer.train()

    # 8. Sauvegarde
    print(f"üíæ Sauvegarde de l'adaptateur dans './{NEW_MODEL_NAME}'...")
    trainer.model.save_pretrained(NEW_MODEL_NAME)
    print("‚úÖ Entra√Ænement termin√© avec succ√®s !")

if __name__ == "__main__":
    train()