version: "3.8"

services:
  # üß† QDRANT (M√©moire Vectorielle)
  qdrant:
    image: qdrant/qdrant:latest
    container_name: raise_qdrant
    # --- INTERRUPTEUR ON/OFF ---
    profiles: ["core", "qdrant"]
    # ---------------------------
    restart: unless-stopped
    ports:
      - "${PORT_QDRANT_HTTP:-6333}:6333"
      - "${PORT_QDRANT_GRPC:-6334}:6334"
    volumes:
      - ${PATH_RAISE_DOMAIN}/qdrant_storage:/qdrant/storage
    environment:
      - QDRANT__SERVICE__GRPC_PORT=6334

  # üìö LEANN (Nouvelle M√©moire L√©g√®re)
  leann:
    # On construit l'image depuis le dossier ./leann (o√π se trouve le Dockerfile)
    build:
      context: ./leann
    container_name: raise_leann
    # --- INTERRUPTEUR ON/OFF ---
    profiles: ["leann"]
    # ---------------------------
    restart: unless-stopped
    ports:
      - "${PORT_LEANN:-8000}:8000"
    volumes:
      # Persistance de l'index LEANN
      - ${PATH_RAISE_DOMAIN}/leann_storage:/data
      # Dossier des documents √† scanner (√† adapter selon votre structure)
      - ${PATH_RAISE_DOMAIN}/documents:/app/documents
    environment:
      # Si LEANN doit utiliser un mod√®le d'embedding sp√©cifique ou une cl√© API
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}

  # ü§ñ LLM LOCAL (Version stable pr√©-compil√©e pour CUDA)
  llm:
    build:
      context: ./llm
      dockerfile: Dockerfile.llm
    container_name: raise_llm
    profiles: ["core", "llm"]
    restart: unless-stopped
    ports:
      - "${PORT_LLM:-8081}:8080"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu, compute]
    volumes:
      - ${PATH_LLM_MODELS}:/models:ro
    environment:
      # On garde tes variables ici pour la propret√©...
      - MODEL=/models/${LLM_MODEL_FILE}
      - HOST=0.0.0.0
      - PORT=8080
      - N_GPU_LAYERS=99
      - N_CTX=4096
      - N_BATCH=512
    # ...MAIS on les injecte ici dans la commande de lancement !
    command: >
      -m /models/${LLM_MODEL_FILE}
      --host 0.0.0.0
      --port 8080
      --n-gpu-layers 99
      --ctx-size 4096
      --batch-size 512
      
  chaincode:
    # On construit depuis la racine (.) pour avoir acc√®s √† 'shared'
    build:
      context: .
      dockerfile: blockchain-engine/chaincode/Dockerfile
    container_name: raise_chaincode
    # --- INTERRUPTEUR ON/OFF ---
    profiles: ["blockchain"]
    # ---------------------------
    restart: unless-stopped
    ports:
      - "9999:9999" # Port gRPC du Chaincode
    environment:
      - CHAINCODE_CC_ID=raise-cc:v1 # ID utilis√© par Fabric
      - CHAINCODE_ADDRESS=0.0.0.0:9999
    networks:
      - raise_net

  # Peer Fabric (N≈ìud de validation)
  # Note: Configuration minimale pour l'instant. N√©cessitera des certificats plus tard.
  peer:
    image: hyperledger/fabric-peer:2.5
    container_name: raise_peer
    profiles: ["blockchain"]
    environment:
      - CORE_PEER_ID=peer0.org1.example.com
      - CORE_PEER_ADDRESS=peer:7051
      - CORE_PEER_LISTENADDRESS=0.0.0.0:7051
      - CORE_PEER_CHAINCODEADDRESS=peer:7052
      - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock
      # Mode Dev pour tester sans certificats complexes au d√©but
      - CORE_PEER_TLS_ENABLED=false
    ports:
      - "7051:7051"
    depends_on:
      - chaincode
    networks:
      - raise_net

networks:
  raise_net:
    driver: bridge
