version: "3.8"

services:
  # ðŸ§  QDRANT (MÃ©moire Vectorielle)
  qdrant:
    image: qdrant/qdrant:latest
    container_name: raise_qdrant
    # --- INTERRUPTEUR ON/OFF ---
    profiles: ["core", "qdrant"] 
    # ---------------------------
    restart: unless-stopped
    ports:
      - "${PORT_QDRANT_HTTP:-6333}:6333"
      - "${PORT_QDRANT_GRPC:-6334}:6334"
    volumes:
      - ${PATH_RAISE_DOMAIN}/qdrant_storage:/qdrant/storage
    environment:
      - QDRANT__SERVICE__GRPC_PORT=6334

  # ðŸ“š LEANN (Nouvelle MÃ©moire LÃ©gÃ¨re)
  leann:
    # On construit l'image depuis le dossier ./leann (oÃ¹ se trouve le Dockerfile)
    build: 
      context: ./leann
    container_name: raise_leann
    # --- INTERRUPTEUR ON/OFF ---
    profiles: ["leann"]
    # ---------------------------
    restart: unless-stopped
    ports:
      - "${PORT_LEANN:-8000}:8000"
    volumes:
      # Persistance de l'index LEANN
      - ${PATH_RAISE_DOMAIN}/leann_storage:/data
      # Dossier des documents Ã  scanner (Ã  adapter selon votre structure)
      - ${PATH_RAISE_DOMAIN}/documents:/app/documents
    environment:
      # Si LEANN doit utiliser un modÃ¨le d'embedding spÃ©cifique ou une clÃ© API
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}

  # ðŸ¤– LLM LOCAL (Llama.cpp Server)
  llm:
    image: ghcr.io/ggerganov/llama.cpp:server-cuda
    container_name: raise_llm
    # --- INTERRUPTEUR ON/OFF ---
    profiles: ["core", "llm"]
    # ---------------------------
    restart: unless-stopped
    ports:
      - "${PORT_LLM:-8081}:8080"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - ${PATH_LLM_MODELS}:/models:ro
    command: >
      -m /models/${LLM_MODEL_FILE}
      -c 8192
      --host 0.0.0.0
      --port 8080
      --n-gpu-layers 99
      --ctx-size 8192
      --batch-size 512